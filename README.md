# `🛠️ DI-Bench`: Benchmarking Large Language Models on Dependency Inference with Testable Repositories

## 🚀 Quick Start

Ensure that Docker engine is installed and running on your machine.

> [!Important]
>
>
> Our testing infrastructure requires [⚙️sysbox](https://github.com/nestybox/sysbox) (a Docker runtime) to be installed on your system to ensure isolation and security.

```shell
# Suggested Python version: 3.10
pip install ".[eval,llm,pattern]"
```

## ⬇️ Download DI-Bench Dataset

[Dataset release page](https://github.com/microsoft/DI-Bench/releases)

## 😎 Evaluation

_Evaluate the correctness of inferred dependencies by checking if the project's tests pass._

```shell
python -m dibench.eval \
    --result_dir [results_dir] \ # the root of results generated by three baseline, json format results evaluating is WIP
    --repo_cache [repo_cache] \
    --dataset_name_or_path [regular_dataset_path/large_dataset_path]
```

## 📃 Documentations
- [Dataset Curation](./dibench/curate/curate.md)
- [Infer Dependencies Using LLMs](./dibench/infer.md)

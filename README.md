# `ğŸ› ï¸ DI-Bench`: Benchmarking Large Language Models on Dependency Inference with Testable Repositories

## ğŸš€ Quick Start

Ensure that Docker engine is installed and running on your machine.

> [!Important]
>
>
> Our testing infrastructure requires [âš™ï¸sysbox](https://github.com/nestybox/sysbox) (a Docker runtime) to be installed on your system to ensure isolation and security.

```shell
# Suggested Python version: 3.10
pip install ".[eval,llm,pattern]"
```

## â¬‡ï¸ Download DI-Bench Dataset

[Dataset release page](https://github.com/microsoft/DI-Bench/releases)

## ğŸ˜ Evaluation

_Evaluate the correctness of inferred dependencies by checking if the project's tests pass._

```shell
python -m dibench.eval \
    --result_dir [results_dir] \ # the root of results generated by three baseline, json format results evaluating is WIP
    --repo_cache [repo_cache] \
    --dataset_name_or_path [regular_dataset_path/large_dataset_path]
```

## ğŸ“ƒ Documentations
- [Dataset Curation](./dibench/curate/curate.md)
- [Infer Dependencies Using LLMs](./dibench/infer.md)
